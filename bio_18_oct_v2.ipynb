{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stephenthomas10/Projects-and-implementations/blob/main/bio_18_oct_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRU8vhAT8v0E"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Base path (update this based on your shortcut location in Drive)\n",
        "base_path = \"/content/drive/MyDrive/test-1,test2 pronation&supanation\"\n",
        "\n",
        "# Subdirectories\n",
        "pronation_arpit = os.path.join(base_path, \"test -1 data sets pronation (arpit)\")\n",
        "supanation_arpit = os.path.join(base_path, \"test -1 data sets supanation (arpit)\")\n",
        "pronation_karthik = os.path.join(base_path, \"test -2 data sets pronation (karthik)\")\n",
        "supanation_karthik = os.path.join(base_path, \"test -2 data sets supanation (karthik)\")\n",
        "\n",
        "# âœ… Add new paths\n",
        "grip_90_degree = os.path.join(base_path, \"90_degree\")\n",
        "grip_180_degree = os.path.join(base_path, \"180_degree\")\n",
        "\n",
        "'''# Verify paths by listing files\n",
        "print(\"Pronation Arpit:\", os.listdir(pronation_arpit))\n",
        "print(\"Supanation Arpit:\", os.listdir(supanation_arpit))\n",
        "print(\"Pronation Karthik:\", os.listdir(pronation_karthik))\n",
        "print(\"Supanation Karthik:\", os.listdir(supanation_karthik))\n",
        "print(\"90 Degree Grip:\", os.listdir(grip_90_degree))\n",
        "print(\"180 Degree Grip:\", os.listdir(grip_180_degree))'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')'''"
      ],
      "metadata": {
        "id": "0vwBmDg989u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''/content/drive/MyDrive/test-1,test2 pronation&supanation\n",
        "/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)\n",
        "/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)\n",
        "/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets pronation (karthik)\n",
        "/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets supanation (karthik)'''"
      ],
      "metadata": {
        "id": "HJKaobvV8-z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/drive/MyDrive/test-1,test2 pronation&supanation\n",
        "/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)\n",
        "/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)\n",
        "/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets pronation (karthik)\n",
        "/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets supanation (karthik)\n"
      ],
      "metadata": {
        "id": "0iaB04tT8nmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nnIWMPQj8lwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def list_files_in_folder(folder_path):\n",
        "    \"\"\"Lists all files within a given folder.\"\"\"\n",
        "    try:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                print(os.path.join(root, file))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Folder not found: {folder_path}\")\n",
        "\n",
        "# Define all folder paths\n",
        "folder_paths = [\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets pronation (karthik)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets supanation (karthik)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/90_degree',  # âœ… Added\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/180_degree'  # âœ… Added\n",
        "]\n",
        "\n",
        "# List files in all folders\n",
        "for folder_path in folder_paths:\n",
        "    print(f\"ðŸ“‚ Files in folder: {folder_path}\")\n",
        "    list_files_in_folder(folder_path)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "-Gkp8RA59OF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas tensorflow matplotlib scikit-learn pillow"
      ],
      "metadata": {
        "id": "rW4lHnSz_sHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def display_csv_data(folder_path):\n",
        "    \"\"\"Displays the CSV data from all files within the folder in a DataFrame.\"\"\"\n",
        "    all_dataframes = []\n",
        "\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".csv\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path)\n",
        "                    all_dataframes.append(df)\n",
        "                except pd.errors.ParserError:\n",
        "                    print(f\"âŒ Error parsing CSV: {file_path}\")\n",
        "\n",
        "    if all_dataframes:\n",
        "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "        print(f\"ðŸ“Š Data from {folder_path}:\\n\", combined_df)\n",
        "    else:\n",
        "        print(f\"âš ï¸ No CSV files found in {folder_path}.\")\n",
        "\n",
        "# Updated folder paths (âœ… Includes 90_degree & 180_degree)\n",
        "folder_paths = [\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets pronation (karthik)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets supanation (karthik)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/90_degree',  # âœ… Added\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/180_degree'  # âœ… Added\n",
        "]\n",
        "\n",
        "for folder_path in folder_paths:\n",
        "    print(f\"ðŸ“‚ CSV Data in folder: {folder_path}\")\n",
        "    display_csv_data(folder_path)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "UA0dftTECBB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def analyze_csv_data(folder_path):\n",
        "    \"\"\"Displays CSV data, counts null values, and identifies their locations.\"\"\"\n",
        "    all_dataframes = []\n",
        "\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".csv\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path)\n",
        "                    all_dataframes.append(df)\n",
        "                except pd.errors.ParserError:\n",
        "                    print(f\"âŒ Error parsing CSV: {file_path}\")\n",
        "\n",
        "    if all_dataframes:\n",
        "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "        # ðŸ“Š Display the DataFrame\n",
        "        print(f\"\\nðŸ“‚ Data from {folder_path} (First 5 rows):\")\n",
        "        print(combined_df.head())\n",
        "\n",
        "        # ðŸ” Count and Display Null Values\n",
        "        null_counts = combined_df.isnull().sum()\n",
        "        total_null = null_counts.sum()\n",
        "        print(\"\\nðŸ”¹ Null Value Counts Per Column:\")\n",
        "        print(null_counts)\n",
        "\n",
        "        # ðŸ“ Display rows with null values\n",
        "        if total_null > 0:\n",
        "            null_locations = combined_df[combined_df.isnull().any(axis=1)]\n",
        "            print(\"\\nðŸ“ Rows with Null Values:\")\n",
        "            print(null_locations)\n",
        "        else:\n",
        "            print(\"\\nâœ… No missing values found.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"âš ï¸ No CSV files found in {folder_path}.\")\n",
        "\n",
        "# Updated folder paths (âœ… Includes 90_degree & 180_degree)\n",
        "folder_paths = [\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets pronation (karthik)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets supanation (karthik)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/90_degree',  # âœ… Added\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/180_degree'  # âœ… Added\n",
        "]\n",
        "\n",
        "for folder_path in folder_paths:\n",
        "    print(f\"\\nðŸ“Š Analyzing CSV Data in folder: {folder_path}\")\n",
        "    analyze_csv_data(folder_path)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "tFFKuAHHCZAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# ðŸ”— Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def analyze_and_process_csv_data(folder_path):\n",
        "    \"\"\"Analyzes CSV data, handles null values, and provides a comprehensive description.\"\"\"\n",
        "    all_dataframes = []\n",
        "\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".csv\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
        "                    all_dataframes.append(df)\n",
        "                except pd.errors.ParserError:\n",
        "                    print(f\"âŒ Error parsing CSV: {file_path}\")\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"âš ï¸ Encoding issue in file: {file_path}\")\n",
        "\n",
        "    if all_dataframes:\n",
        "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "        # ðŸ“Š Display the first 5 rows for preview\n",
        "        print(f\"\\nðŸ“‚ Data from {folder_path} (First 5 rows):\")\n",
        "        print(combined_df.head())\n",
        "\n",
        "        # ðŸ” Count and Display Null Values\n",
        "        null_counts = combined_df.isnull().sum()\n",
        "        total_null = null_counts.sum()\n",
        "        print(\"\\nðŸ”¹ Null Value Counts Per Column:\")\n",
        "        print(null_counts)\n",
        "\n",
        "        # ðŸ“ Display rows with null values\n",
        "        if total_null > 0:\n",
        "            null_locations = combined_df[combined_df.isnull().any(axis=1)]\n",
        "            print(\"\\nðŸ“ Rows with Null Values:\")\n",
        "            print(null_locations)\n",
        "        else:\n",
        "            print(\"\\nâœ… No missing values found.\")\n",
        "\n",
        "        # ðŸ›  Fill null values using forward fill\n",
        "        combined_df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "        # ðŸ“Š Generate a detailed statistical description\n",
        "        print(\"\\nðŸ“– Data Summary:\")\n",
        "        print(combined_df.describe(include='all'))  # Include all types of columns\n",
        "\n",
        "        # ðŸ— Display dataset structure\n",
        "        print(\"\\nðŸ“Œ Data Structure & Column Info:\")\n",
        "        combined_df.info()\n",
        "\n",
        "        # ðŸ”„ Check for duplicate rows\n",
        "        duplicates = combined_df.duplicated().sum()\n",
        "        print(f\"\\nðŸ” Duplicate Rows Found: {duplicates}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"âš ï¸ No CSV files found in {folder_path}.\")\n",
        "\n",
        "# âœ… Updated folder paths (Includes 90Â° & 180Â° datasets)\n",
        "folder_paths = [\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets pronation (karthik)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets supanation (karthik)',\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/90_degree',  # âœ… Added\n",
        "    '/content/drive/MyDrive/test-1,test2 pronation&supanation/180_degree'  # âœ… Added\n",
        "]\n",
        "\n",
        "for folder_path in folder_paths:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"ðŸ“Š Analyzing and Processing CSV Data in folder: {folder_path}\")\n",
        "    analyze_and_process_csv_data(folder_path)\n",
        "    print(\"=\"*80 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "pku39DtPCsKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define dataset paths\n",
        "dataset_paths = {\n",
        "    'pronation': [\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 8.csv',\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 9.csv'\n",
        "    ],\n",
        "    'supination': [\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 10.csv',\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 11.csv'\n",
        "    ],\n",
        "    'pronation_image': '/content/drive/MyDrive/test-1,test2 pronation&supanation/pronation.jpeg',\n",
        "    'supination_image': '/content/drive/MyDrive/test-1,test2 pronation&supanation/supination.jpeg'\n",
        "}\n",
        "\n",
        "# Load data with handling for missing columns\n",
        "def load_data():\n",
        "    data, labels = [], []\n",
        "    for label, file_list in enumerate([dataset_paths['pronation'], dataset_paths['supination']]):\n",
        "        for file_path in file_list:\n",
        "            df = pd.read_csv(file_path, dtype=str)\n",
        "            df = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "            # Drop rows where all values are NaN\n",
        "            df.dropna(how='all', inplace=True)\n",
        "\n",
        "            # Drop NaN rows from the first column if it exists\n",
        "            first_col = df.columns[0]\n",
        "            df.dropna(subset=[first_col], inplace=True)\n",
        "\n",
        "            if not df.empty:\n",
        "                data.append(df.values)\n",
        "                labels.append(label)\n",
        "\n",
        "    if len(data) == 0:\n",
        "        raise ValueError(\"No valid data found in CSV files.\")\n",
        "\n",
        "    # Pad sequences to the longest sequence\n",
        "    max_timesteps = max(len(d) for d in data)\n",
        "    data = pad_sequences(data, maxlen=max_timesteps, padding='post', dtype='float32')\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "# Standardize & Split data\n",
        "def preprocess_data(data, labels):\n",
        "    num_samples, num_timesteps, num_features = data.shape\n",
        "    data = data.reshape((num_samples, num_timesteps * num_features))\n",
        "\n",
        "    global scaler\n",
        "    scaler = StandardScaler()\n",
        "    data = scaler.fit_transform(data)\n",
        "\n",
        "    data = data.reshape((num_samples, num_timesteps, num_features))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# LSTM Model\n",
        "def build_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(0.5),\n",
        "        LSTM(64),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train Model\n",
        "def train_model(model, X_train, y_train):\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "    return history\n",
        "\n",
        "# Prediction Function\n",
        "def predict_gesture(model, csv_file_path):\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "    df = df.apply(pd.to_numeric, errors='coerce')\n",
        "    df.dropna(how='all', inplace=True)\n",
        "\n",
        "    data = pad_sequences([df.values], maxlen=X_train.shape[1], padding='post', dtype='float32')\n",
        "    data = scaler.transform(data.reshape(1, -1)).reshape(1, X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    prediction = model.predict(data)\n",
        "    predicted_label = \"Pronation\" if prediction >= 0.5 else \"Supination\"\n",
        "\n",
        "    print(f\"Predicted Gesture: {predicted_label}\")\n",
        "    img = Image.open(dataset_paths[f'{predicted_label.lower()}_image'])\n",
        "    display(img)\n",
        "\n",
        "# Execute Training\n",
        "data, labels = load_data()\n",
        "X_train, X_test, y_train, y_test = preprocess_data(data, labels)\n",
        "\n",
        "model = build_model((X_train.shape[1], X_train.shape[2]))\n",
        "history = train_model(model, X_train, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Upload CSV for Prediction\n",
        "print(\"Upload a CSV file to predict gesture:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Processing file: {filename}\")\n",
        "    predict_gesture(model, filename)\n"
      ],
      "metadata": {
        "id": "CyOsXflx9y9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/test-1,test2 pronation&supanation/\"  # Update with your base path\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    level = root.replace(dataset_path, \"\").count(os.sep)\n",
        "    indent = \" \" * (level * 4)\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    sub_indent = \" \" * ((level + 1) * 4)\n",
        "    for f in files:\n",
        "        print(f\"{sub_indent}{f}\")\n"
      ],
      "metadata": {
        "id": "mhPmZqJ8lOr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 8.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(df.head())  # Check the content of the first few rows\n",
        "print(df.dtypes)  # Check the data types of columns\n",
        "print(df.isna().sum())  # Check if NaN values exist\n"
      ],
      "metadata": {
        "id": "ODusem7geS-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas tensorflow matplotlib scikit-learn pillow gradio"
      ],
      "metadata": {
        "id": "4GViC2WrOs-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# Dictionary to hold the paths for the pronation, supination datasets and images\n",
        "dataset_paths = {\n",
        "    'pronation': [\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 8.csv',\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 9.csv',\n",
        "        # Add all other pronation CSV paths here...\n",
        "    ],\n",
        "    'supination': [\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 8.csv',\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 9.csv',\n",
        "        # Add all other supination CSV paths here...\n",
        "    ],\n",
        "    'pronation_image': '/content/drive/MyDrive/test-1,test2 pronation&supanation/pronation.jpeg',\n",
        "    'supination_image': '/content/drive/MyDrive/test-1,test2 pronation&supanation/supination.jpeg'\n",
        "}\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_and_preprocess_data(dataset_paths):\n",
        "    # Initialize empty lists for features and labels\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Load pronation datasets\n",
        "    for file_path in dataset_paths['pronation']:\n",
        "        data = pd.read_csv(file_path)\n",
        "        X.append(data.values)\n",
        "        y.append(0)  # 0 for pronation\n",
        "\n",
        "    # Load supination datasets\n",
        "    for file_path in dataset_paths['supination']:\n",
        "        data = pd.read_csv(file_path)\n",
        "        X.append(data.values)\n",
        "        y.append(1)  # 1 for supination\n",
        "\n",
        "    # Convert lists to numpy arrays for model processing\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Flatten the feature arrays and normalize (standardization)\n",
        "    num_samples, num_timesteps, num_features = X.shape\n",
        "    X = X.reshape(num_samples, num_timesteps * num_features)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Reshape back to original structure for LSTM input (samples, timesteps, features)\n",
        "    X = X.reshape(num_samples, num_timesteps, num_features)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Load the datasets\n",
        "X, y = load_and_preprocess_data(dataset_paths)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the LSTM model\n",
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification output\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Get input shape for the model\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "# Instantiate and train the model\n",
        "model = build_lstm_model(input_shape)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Function to make predictions and display the corresponding image\n",
        "def predict_and_display(csv_file):\n",
        "    # Load the CSV file uploaded by the user\n",
        "    data = pd.read_csv(csv_file)\n",
        "    data = data.values\n",
        "    data = data.reshape(1, -1)  # Reshape for model input\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    data = scaler.fit_transform(data)\n",
        "\n",
        "    # Reshape to LSTM input format\n",
        "    num_timesteps = X_train.shape[1]\n",
        "    num_features = X_train.shape[2]\n",
        "    data = data.reshape(1, num_timesteps, num_features)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(data)[0][0]\n",
        "\n",
        "    # If prediction is close to 0, it is pronation; if close to 1, it is supination\n",
        "    if prediction < 0.5:\n",
        "        img_path = dataset_paths['pronation_image']\n",
        "        result = 'Pronation'\n",
        "    else:\n",
        "        img_path = dataset_paths['supination_image']\n",
        "        result = 'Supination'\n",
        "\n",
        "    # Load and display the image\n",
        "    img = Image.open(img_path)\n",
        "    return img, result\n",
        "\n",
        "# Create a Gradio interface for user interaction\n",
        "def predict_from_csv(csv_file):\n",
        "    img, result = predict_and_display(csv_file)\n",
        "    return img, f\"Predicted Gesture: {result}\"\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_from_csv,\n",
        "    inputs=gr.inputs.File(label=\"Upload EMG CSV File\"),\n",
        "    outputs=[gr.outputs.Image(type=\"pil\", label=\"Gesture Image\"), gr.outputs.Textbox(label=\"Prediction\")],\n",
        "    title=\"EMG Gesture Classifier\",\n",
        "    description=\"Upload a CSV file containing EMG data to predict if the gesture is pronation or supination.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "id": "YVlPz8M8AoeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# Dictionary to hold the paths for the pronation, supination datasets and images\n",
        "dataset_paths = {\n",
        "    'pronation': [\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 8.csv',\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 9.csv',\n",
        "        # Add all other pronation CSV paths here...\n",
        "    ],\n",
        "    'supination': [\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 8.csv',\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 9.csv',\n",
        "        # Add all other supination CSV paths here...\n",
        "    ],\n",
        "    'pronation_image': '/content/drive/MyDrive/test-1,test2 pronation&supanation/pronation.jpeg',\n",
        "    'supination_image': '/content/drive/MyDrive/test-1,test2 pronation&supanation/supination.jpeg'\n",
        "}\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_and_preprocess_data(dataset_paths):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Load pronation datasets\n",
        "    for file_path in dataset_paths['pronation']:\n",
        "        data = pd.read_csv(file_path)\n",
        "        X.append(data.values)\n",
        "        y.append(0)  # 0 for pronation\n",
        "\n",
        "    # Load supination datasets\n",
        "    for file_path in dataset_paths['supination']:\n",
        "        data = pd.read_csv(file_path)\n",
        "        X.append(data.values)\n",
        "        y.append(1)  # 1 for supination\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Flatten the feature arrays and normalize\n",
        "    num_samples, num_timesteps, num_features = X.shape\n",
        "    X = X.reshape(num_samples, num_timesteps * num_features)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Reshape back to original structure for LSTM input (samples, timesteps, features)\n",
        "    X = X.reshape(num_samples, num_timesteps, num_features)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Load the datasets\n",
        "X, y = load_and_preprocess_data(dataset_paths)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the LSTM model\n",
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification output\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Get input shape for the model\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "# Instantiate and train the model\n",
        "model = build_lstm_model(input_shape)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Function to make predictions and display the corresponding image\n",
        "def predict_and_display(csv_file):\n",
        "    # Load the CSV file uploaded by the user\n",
        "    data = pd.read_csv(csv_file)\n",
        "    data = data.values\n",
        "\n",
        "    # Check the number of features and reshape accordingly\n",
        "    num_timesteps = X_train.shape[1]\n",
        "    num_features = X_train.shape[2]\n",
        "\n",
        "    # Ensure the data is compatible with the LSTM input shape\n",
        "    if data.shape[1] != num_features:\n",
        "        raise ValueError(f\"Expected {num_features} features, got {data.shape[1]} features.\")\n",
        "\n",
        "    # Standardize the data\n",
        "    data = data.reshape(1, -1)  # Flatten for standardization\n",
        "    scaler = StandardScaler()\n",
        "    data = scaler.fit_transform(data)\n",
        "\n",
        "    # Reshape to LSTM input format\n",
        "    data = data.reshape(1, num_timesteps, num_features)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(data)[0][0]\n",
        "\n",
        "    # Determine the predicted class\n",
        "    if prediction < 0.5:\n",
        "        img_path = dataset_paths['pronation_image']\n",
        "        result = 'Pronation'\n",
        "    else:\n",
        "        img_path = dataset_paths['supination_image']\n",
        "        result = 'Supination'\n",
        "\n",
        "    # Load and display the image\n",
        "    img = Image.open(img_path)\n",
        "    return img, result\n",
        "\n",
        "# Create a Gradio interface for user interaction\n",
        "def predict_from_csv(csv_file):\n",
        "    img, result = predict_and_display(csv_file)\n",
        "    return img, f\"Predicted Gesture: {result}\"\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_from_csv,\n",
        "    inputs=gr.File(label=\"Upload EMG CSV File\"),\n",
        "    outputs=[gr.Image(type=\"pil\", label=\"Gesture Image\"), gr.Textbox(label=\"Prediction\")],\n",
        "    title=\"EMG Gesture Classifier\",\n",
        "    description=\"Upload a CSV file containing EMG data to predict if the gesture is pronation or supination.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "id": "eqb8YfMuOnL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas tensorflow scikit-learn pillow gradio"
      ],
      "metadata": {
        "id": "HyM393YEQSgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# Define paths for the dataset and images\n",
        "dataset_paths = {\n",
        "    'pronation': [\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 8.csv',\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 9.csv',\n",
        "        # Add other pronation CSV paths here...\n",
        "    ],\n",
        "    'supination': [\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 8.csv',\n",
        "        '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 9.csv',\n",
        "        # Add other supination CSV paths here...\n",
        "    ],\n",
        "    'pronation_image': '/content/drive/MyDrive/test-1,test2 pronation&supanation/pronation.jpeg',\n",
        "    'supination_image': '/content/drive/MyDrive/test-1,test2 pronation&supanation/supination.jpeg'\n",
        "}\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_and_preprocess_data(dataset_paths):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Load pronation datasets\n",
        "    for file_path in dataset_paths['pronation']:\n",
        "        try:\n",
        "            data = pd.read_csv(file_path)\n",
        "            emg_signals = data['emg_signals'].values\n",
        "            gesture_type = data['gesture_type'].values\n",
        "\n",
        "            # Append features and labels\n",
        "            num_samples = len(emg_signals) // 100  # Assuming each sample has 100 timesteps\n",
        "            for i in range(num_samples):\n",
        "                start_index = i * 100\n",
        "                end_index = start_index + 100\n",
        "                X.append(emg_signals[start_index:end_index])\n",
        "                y.append(0)  # 0 for pronation\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    # Load supination datasets\n",
        "    for file_path in dataset_paths['supination']:\n",
        "        try:\n",
        "            data = pd.read_csv(file_path)\n",
        "            emg_signals = data['emg_signals'].values\n",
        "            gesture_type = data['gesture_type'].values\n",
        "\n",
        "            num_samples = len(emg_signals) // 100  # Assuming each sample has 100 timesteps\n",
        "            for i in range(num_samples):\n",
        "                start_index = i * 100\n",
        "                end_index = start_index + 100\n",
        "                X.append(emg_signals[start_index:end_index])\n",
        "                y.append(1)  # 1 for supination\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    if not X or not y:\n",
        "        raise ValueError(\"No valid data was loaded. Check your dataset paths and contents.\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Reshape data for LSTM\n",
        "    X = X.reshape(X.shape[0], 100, 1)  # Reshape to (samples, timesteps, features)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Load the datasets\n",
        "X, y = load_and_preprocess_data(dataset_paths)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the LSTM model\n",
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification output\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Get input shape for the model\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "# Instantiate and train the model\n",
        "model = build_lstm_model(input_shape)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Function to make predictions and display the corresponding image\n",
        "def predict_and_display(csv_file):\n",
        "    # Load the CSV file uploaded by the user\n",
        "    data = pd.read_csv(csv_file)\n",
        "\n",
        "    # Extract EMG signals and reshape for prediction\n",
        "    emg_signals = data['emg_signals'].values\n",
        "\n",
        "    # Ensure the length of signals is appropriate for prediction\n",
        "    if len(emg_signals) < 100:\n",
        "        raise ValueError(\"Input data must have at least 100 data points.\")\n",
        "\n",
        "    # Truncate or pad the data as necessary\n",
        "    if len(emg_signals) > 100:\n",
        "        emg_signals = emg_signals[:100]\n",
        "    else:\n",
        "        emg_signals = np.pad(emg_signals, (0, 100 - len(emg_signals)), 'constant')\n",
        "\n",
        "    # Reshape for LSTM input\n",
        "    data = emg_signals.reshape(1, 100, 1)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(data)[0][0]\n",
        "\n",
        "    # Determine the predicted class\n",
        "    if prediction < 0.5:\n",
        "        img_path = dataset_paths['pronation_image']\n",
        "        result = 'Pronation'\n",
        "    else:\n",
        "        img_path = dataset_paths['supination_image']\n",
        "        result = 'Supination'\n",
        "\n",
        "    # Load and display the image\n",
        "    img = Image.open(img_path)\n",
        "    return img, result\n",
        "\n",
        "# Create a Gradio interface for user interaction\n",
        "def predict_from_csv(csv_file):\n",
        "    img, result = predict_and_display(csv_file)\n",
        "    return img, f\"Predicted Gesture: {result}\"\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_from_csv,\n",
        "    inputs=gr.File(label=\"Upload EMG CSV File\"),\n",
        "    outputs=[gr.Image(type=\"pil\", label=\"Gesture Image\"), gr.Textbox(label=\"Prediction\")],\n",
        "    title=\"EMG Gesture Classifier\",\n",
        "    description=\"Upload a CSV file containing EMG data to predict if the gesture is pronation or supination.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "id": "74qH6WRhOn7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "class EMGGestureClassifier:\n",
        "    def __init__(self):\n",
        "        self.dataset_paths = {\n",
        "            'pronation': [\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 8.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 9.csv'\n",
        "            ],\n",
        "            'supination': [\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 8.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 9.csv'\n",
        "            ],\n",
        "            'images': {\n",
        "                'pronation': '/content/drive/MyDrive/test-1,test2 pronation&supanation/pronation.jpeg',\n",
        "                'supination': '/content/drive/MyDrive/test-1,test2 pronation&supanation/supination.jpeg'\n",
        "            }\n",
        "        }\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.sequence_length = 100\n",
        "        self.model_path = 'emg_gesture_model.h5'\n",
        "        self.scaler_path = 'emg_scaler.pkl'\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "        \"\"\"Load and preprocess EMG data from all sources\"\"\"\n",
        "        all_data = []\n",
        "        all_labels = []\n",
        "\n",
        "        # Process each gesture type\n",
        "        for gesture_type, file_paths in self.dataset_paths.items():\n",
        "            if gesture_type == 'images':\n",
        "                continue\n",
        "\n",
        "            for file_path in file_paths:\n",
        "                try:\n",
        "                    # Load data\n",
        "                    df = pd.read_csv(file_path)\n",
        "\n",
        "                    # Clean data\n",
        "                    df['emg_signals'] = pd.to_numeric(df['emg_signals'], errors='coerce')\n",
        "                    df = df.dropna(subset=['emg_signals'])(inplace=True)\n",
        "\n",
        "                    # Create sequences\n",
        "                    signals = df['emg_signals'].values\n",
        "                    for i in range(0, len(signals) - self.sequence_length + 1, 50):  # 50% overlap\n",
        "                        sequence = signals[i:i + self.sequence_length]\n",
        "                        all_data.append(sequence)\n",
        "                        all_labels.append(1 if gesture_type == 'supination' else 0)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        X = np.array(all_data)\n",
        "        y = np.array(all_labels)\n",
        "\n",
        "        # Normalize the data\n",
        "        X = self.scaler.fit_transform(X.reshape(-1, 1)).reshape(X.shape)\n",
        "\n",
        "        # Reshape for LSTM (samples, timesteps, features)\n",
        "        X = X.reshape(X.shape[0], self.sequence_length, 1)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Create an LSTM model with advanced architecture\"\"\"\n",
        "        model = Sequential([\n",
        "            LSTM(128, input_shape=(self.sequence_length, 1), return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            LSTM(64, return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            LSTM(32),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(16, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_model(self, X, y, epochs=100):\n",
        "        \"\"\"Train the model with advanced techniques for high accuracy\"\"\"\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Define callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=20,\n",
        "                restore_best_weights=True,\n",
        "                mode='max'\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=10,\n",
        "                min_lr=1e-6\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Build and train model\n",
        "        self.model = self.build_model()\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=epochs,\n",
        "            batch_size=32,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        test_loss, test_accuracy = self.model.evaluate(X_test, y_test)\n",
        "        print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "        # Save model and scaler\n",
        "        self.save_model()\n",
        "\n",
        "        return history, (X_test, y_test)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the trained model and scaler\"\"\"\n",
        "        self.model.save(self.model_path)\n",
        "        import joblib\n",
        "        joblib.dump(self.scaler, self.scaler_path)\n",
        "\n",
        "    def load_saved_model(self):\n",
        "        \"\"\"Load a previously saved model and scaler\"\"\"\n",
        "        import joblib\n",
        "        self.model = load_model(self.model_path)\n",
        "        self.scaler = joblib.load(self.scaler_path)\n",
        "\n",
        "    def predict_gesture(self, csv_file):\n",
        "        \"\"\"Predict gesture from uploaded CSV file\"\"\"\n",
        "        try:\n",
        "            # Load and preprocess the input data\n",
        "            df = pd.read_csv(csv_file.name)\n",
        "            signals = pd.to_numeric(df['emg_signals'], errors='coerce').dropna().values\n",
        "\n",
        "            # Ensure we have enough data points\n",
        "            if len(signals) < self.sequence_length:\n",
        "                return None, \"Error: Not enough data points in the input file\"\n",
        "\n",
        "            # Take the middle sequence to avoid edge effects\n",
        "            start_idx = (len(signals) - self.sequence_length) // 2\n",
        "            sequence = signals[start_idx:start_idx + self.sequence_length]\n",
        "\n",
        "            # Normalize and reshape\n",
        "            sequence = self.scaler.transform(sequence.reshape(-1, 1))\n",
        "            sequence = sequence.reshape(1, self.sequence_length, 1)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = self.model.predict(sequence)[0][0]\n",
        "\n",
        "            # Load appropriate image\n",
        "            if prediction < 0.5:\n",
        "                gesture = \"Pronation\"\n",
        "                image_path = self.dataset_paths['images']['pronation']\n",
        "            else:\n",
        "                gesture = \"Supination\"\n",
        "                image_path = self.dataset_paths['images']['supination']\n",
        "\n",
        "            # Load and return image\n",
        "            img = Image.open(image_path)\n",
        "            confidence = prediction if prediction >= 0.5 else 1 - prediction\n",
        "\n",
        "            return img, f\"Predicted Gesture: {gesture} (Confidence: {confidence*100:.2f}%)\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"Error processing file: {str(e)}\"\n",
        "\n",
        "    def plot_training_history(self, history):\n",
        "        \"\"\"Plot training metrics\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Plot accuracy\n",
        "        ax1.plot(history.history['accuracy'])\n",
        "        ax1.plot(history.history['val_accuracy'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Train', 'Validation'])\n",
        "\n",
        "        # Plot loss\n",
        "        ax2.plot(history.history['loss'])\n",
        "        ax2.plot(history.history['val_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Train', 'Validation'])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "    # Initialize classifier\n",
        "    classifier = EMGGestureClassifier()\n",
        "\n",
        "    # Check if saved model exists\n",
        "    if os.path.exists(classifier.model_path):\n",
        "        print(\"Loading saved model...\")\n",
        "        classifier.load_saved_model()\n",
        "    else:\n",
        "        print(\"Training new model...\")\n",
        "        # Load and preprocess data\n",
        "        X, y = classifier.load_and_preprocess_data()\n",
        "\n",
        "        # Train model\n",
        "        history, test_data = classifier.train_model(X, y)\n",
        "\n",
        "        # Plot training history\n",
        "        classifier.plot_training_history(history)\n",
        "\n",
        "    # Create Gradio interface\n",
        "    interface = gr.Interface(\n",
        "        fn=classifier.predict_gesture,\n",
        "        inputs=gr.File(label=\"Upload EMG CSV File\"),\n",
        "        outputs=[\n",
        "            gr.Image(type=\"pil\", label=\"Gesture Image\"),\n",
        "            gr.Textbox(label=\"Prediction\")\n",
        "        ],\n",
        "        title=\"EMG Gesture Classifier\",\n",
        "        description=\"Upload a CSV file containing EMG data to predict if the gesture is pronation or supination.\"\n",
        "    )\n",
        "\n",
        "    # Launch the interface\n",
        "    interface.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "D38SVEjmPh9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "class EMGGestureClassifier:\n",
        "    def __init__(self):\n",
        "        self.dataset_paths = {\n",
        "            'pronation': [\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 8.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 9.csv'\n",
        "            ],\n",
        "            'supination': [\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 8.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 9.csv'\n",
        "            ],\n",
        "            'images': {\n",
        "                'pronation': '/content/drive/MyDrive/test-1,test2 pronation&supanation/pronation.jpeg',\n",
        "                'supination': '/content/drive/MyDrive/test-1,test2 pronation&supanation/supination.jpeg'\n",
        "            }\n",
        "        }\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.sequence_length = 100\n",
        "        self.model_path = 'emg_gesture_model.h5'\n",
        "        self.scaler_path = 'emg_scaler.pkl'\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "        \"\"\"Load and preprocess EMG data from all sources\"\"\"\n",
        "        all_data = []\n",
        "        all_labels = []\n",
        "\n",
        "        for gesture_type, file_paths in self.dataset_paths.items():\n",
        "            if gesture_type == 'images':\n",
        "                continue\n",
        "\n",
        "            for file_path in file_paths:\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path)\n",
        "                    df['emg_signals'] = pd.to_numeric(df['emg_signals'], errors='coerce')\n",
        "                    df = df.dropna(subset=['emg_signals'])(inplace=True)\n",
        "\n",
        "                    signals = df['emg_signals'].values\n",
        "                    for i in range(0, len(signals) - self.sequence_length + 1, 50):  # 50% overlap\n",
        "                        sequence = signals[i:i + self.sequence_length]\n",
        "\n",
        "                        # Calculate IQR for the sequence\n",
        "                        q1, q3 = np.percentile(sequence, [25, 75])\n",
        "                        iqr = q3 - q1\n",
        "\n",
        "                        # Add IQR as a feature\n",
        "                        sequence_with_iqr = np.append(sequence, iqr)\n",
        "\n",
        "                        all_data.append(sequence_with_iqr)\n",
        "                        all_labels.append(1 if gesture_type == 'supination' else 0)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        X = np.array(all_data)\n",
        "        y = np.array(all_labels)\n",
        "\n",
        "        # Normalize the data\n",
        "        X = self.scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "\n",
        "        # Reshape for LSTM (samples, timesteps, features)\n",
        "        X = X.reshape(X.shape[0], self.sequence_length + 1, 1)  # +1 for IQR feature\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Create an LSTM model with advanced architecture\"\"\"\n",
        "        model = Sequential([\n",
        "            LSTM(128, input_shape=(self.sequence_length + 1, 1), return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            LSTM(64, return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            LSTM(32),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(16, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_model(self, X, y, epochs=100):\n",
        "        \"\"\"Train the model with advanced techniques for high accuracy\"\"\"\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=20,\n",
        "                restore_best_weights=True,\n",
        "                mode='max'\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=10,\n",
        "                min_lr=1e-6\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        self.model = self.build_model()\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=epochs,\n",
        "            batch_size=32,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        test_loss, test_accuracy = self.model.evaluate(X_test, y_test)\n",
        "        print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "        self.save_model()\n",
        "\n",
        "        return history, (X_test, y_test)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the trained model and scaler\"\"\"\n",
        "        self.model.save(self.model_path)\n",
        "        import joblib\n",
        "        joblib.dump(self.scaler, self.scaler_path)\n",
        "\n",
        "    def load_saved_model(self):\n",
        "        \"\"\"Load a previously saved model and scaler\"\"\"\n",
        "        import joblib\n",
        "        self.model = load_model(self.model_path)\n",
        "        self.scaler = joblib.load(self.scaler_path)\n",
        "\n",
        "    def predict_gesture(self, csv_file):\n",
        "        \"\"\"Predict gesture from uploaded CSV file\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file.name)\n",
        "            signals = pd.to_numeric(df['emg_signals'], errors='coerce').dropna().values\n",
        "\n",
        "            if len(signals) < self.sequence_length:\n",
        "                return None, \"Error: Not enough data points in the input file\"\n",
        "\n",
        "            start_idx = (len(signals) - self.sequence_length) // 2\n",
        "            sequence = signals[start_idx:start_idx + self.sequence_length]\n",
        "\n",
        "            # Calculate IQR\n",
        "            q1, q3 = np.percentile(sequence, [25, 75])\n",
        "            iqr = q3 - q1\n",
        "\n",
        "            # Add IQR to the sequence\n",
        "            sequence_with_iqr = np.append(sequence, iqr)\n",
        "\n",
        "            # Normalize and reshape\n",
        "            sequence_with_iqr = self.scaler.transform(sequence_with_iqr.reshape(1, -1))\n",
        "            sequence_with_iqr = sequence_with_iqr.reshape(1, self.sequence_length + 1, 1)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = self.model.predict(sequence_with_iqr)[0][0]\n",
        "\n",
        "            # Use IQR information to refine prediction\n",
        "            if 0 <= iqr <= 2:\n",
        "                gesture = \"Pronation\"\n",
        "                confidence = max(1 - prediction, 0.9)  # Increase confidence for pronation\n",
        "            elif iqr == 0:\n",
        "                gesture = \"Supination\"\n",
        "                confidence = max(prediction, 0.9)  # Increase confidence for supination\n",
        "            else:\n",
        "                gesture = \"Supination\" if prediction >= 0.5 else \"Pronation\"\n",
        "                confidence = prediction if prediction >= 0.5 else 1 - prediction\n",
        "\n",
        "            # Load appropriate image\n",
        "            image_path = self.dataset_paths['images'][gesture.lower()]\n",
        "            img = Image.open(image_path)\n",
        "\n",
        "            return img, f\"Predicted Gesture: {gesture} (Confidence: {confidence*100:.2f}%)\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"Error processing file: {str(e)}\"\n",
        "\n",
        "    def plot_training_history(self, history):\n",
        "        \"\"\"Plot training metrics\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        ax1.plot(history.history['accuracy'])\n",
        "        ax1.plot(history.history['val_accuracy'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Train', 'Validation'])\n",
        "\n",
        "        ax2.plot(history.history['loss'])\n",
        "        ax2.plot(history.history['val_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Train', 'Validation'])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "    classifier = EMGGestureClassifier()\n",
        "\n",
        "    if os.path.exists(classifier.model_path):\n",
        "        print(\"Loading saved model...\")\n",
        "        classifier.load_saved_model()\n",
        "    else:\n",
        "        print(\"Training new model...\")\n",
        "        X, y = classifier.load_and_preprocess_data()\n",
        "        history, test_data = classifier.train_model(X, y)\n",
        "        classifier.plot_training_history(history)\n",
        "\n",
        "    interface = gr.Interface(\n",
        "        fn=classifier.predict_gesture,\n",
        "        inputs=gr.File(label=\"Upload EMG CSV File\"),\n",
        "        outputs=[\n",
        "            gr.Image(type=\"pil\", label=\"Gesture Image\"),\n",
        "            gr.Textbox(label=\"Prediction\")\n",
        "        ],\n",
        "        title=\"EMG Gesture Classifier\",\n",
        "        description=\"Upload a CSV file containing EMG data to predict if the gesture is pronation or supination.\"\n",
        "    )\n",
        "\n",
        "    interface.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "U4TDP3egVT31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "class EMGGestureClassifier:\n",
        "    def __init__(self):\n",
        "        self.dataset_paths = {\n",
        "            'pronation': [\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 8.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets pronation (karthik)/karthik pronation - test 1.csv'\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 9.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets pronation (karthik)/karthik pronation - test 3.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets pronation (arpit)/pronation test 12.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets pronation (karthik)/karthik pronation - test 5.csv'\n",
        "\n",
        "            ],\n",
        "            'supination': [\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 8.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets supanation (karthik)/karthik supantion - test 1.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 9.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets supanation (karthik)/karthik supantion - test 5.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -1 data sets supanation (arpit)/supanation test 13.csv',\n",
        "                '/content/drive/MyDrive/test-1,test2 pronation&supanation/test -2 data sets supanation (karthik)/karthik supantion - test 7.csv'\n",
        "\n",
        "\n",
        "            ],\n",
        "            'images': {\n",
        "                'pronation': '/content/drive/MyDrive/test-1,test2 pronation&supanation/pronation.jpeg',\n",
        "                'supination': '/content/drive/MyDrive/test-1,test2 pronation&supanation/supination.jpeg'\n",
        "            }\n",
        "        }\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.sequence_length = 100\n",
        "        self.model_path = 'emg_gesture_model.h5'\n",
        "        self.scaler_path = 'emg_scaler.pkl'\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "        \"\"\"Load and preprocess EMG data from all sources\"\"\"\n",
        "        all_data = []\n",
        "        all_labels = []\n",
        "\n",
        "        for gesture_type, file_paths in self.dataset_paths.items():\n",
        "            if gesture_type == 'images':\n",
        "                continue\n",
        "\n",
        "            for file_path in file_paths:\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path)\n",
        "                    df['emg_signals'] = pd.to_numeric(df['emg_signals'], errors='coerce')\n",
        "                    df.dropna(subset=['emg_signals'])(inplace=True)\n",
        "\n",
        "                    signals = df['emg_signals'].values\n",
        "                    for i in range(0, len(signals) - self.sequence_length + 1, 50):  # 50% overlap\n",
        "                        sequence = signals[i:i + self.sequence_length]\n",
        "\n",
        "                        # Calculate IQR for the sequence\n",
        "                        q1, q3 = np.percentile(sequence, [25, 75])\n",
        "                        iqr = q3 - q1\n",
        "\n",
        "                        # Add IQR as a feature\n",
        "                        sequence_with_iqr = np.append(sequence, iqr)\n",
        "\n",
        "                        all_data.append(sequence_with_iqr)\n",
        "                        all_labels.append(1 if gesture_type == 'supination' else 0)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        X = np.array(all_data)\n",
        "        y = np.array(all_labels)\n",
        "\n",
        "        # Normalize the data\n",
        "        X = self.scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "\n",
        "        # Reshape for LSTM (samples, timesteps, features)\n",
        "        X = X.reshape(X.shape[0], self.sequence_length + 1, 1)  # +1 for IQR feature\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Create an LSTM model with advanced architecture\"\"\"\n",
        "        model = Sequential([\n",
        "            LSTM(128, input_shape=(self.sequence_length + 1, 1), return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            LSTM(64, return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            LSTM(32),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(16, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_model(self, X, y, epochs=100):\n",
        "        \"\"\"Train the model with advanced techniques for high accuracy\"\"\"\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=20,\n",
        "                restore_best_weights=True,\n",
        "                mode='max'\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=10,\n",
        "                min_lr=1e-6\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        self.model = self.build_model()\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=epochs,\n",
        "            batch_size=32,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        test_loss, test_accuracy = self.model.evaluate(X_test, y_test)\n",
        "        print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "        self.save_model()\n",
        "\n",
        "        return history, (X_test, y_test)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the trained model and scaler\"\"\"\n",
        "        self.model.save(self.model_path)\n",
        "        import joblib\n",
        "        joblib.dump(self.scaler, self.scaler_path)\n",
        "\n",
        "    def load_saved_model(self):\n",
        "        \"\"\"Load a previously saved model and scaler\"\"\"\n",
        "        import joblib\n",
        "        self.model = load_model(self.model_path)\n",
        "        self.scaler = joblib.load(self.scaler_path)\n",
        "\n",
        "    def predict_gesture(self, csv_file):\n",
        "        \"\"\"Predict gesture from uploaded CSV file\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file.name)\n",
        "            signals = pd.to_numeric(df['emg_signals'], errors='coerce').dropna().values\n",
        "\n",
        "            if len(signals) < self.sequence_length:\n",
        "                return None, \"Error: Not enough data points in the input file\"\n",
        "\n",
        "            start_idx = (len(signals) - self.sequence_length) // 2\n",
        "            sequence = signals[start_idx:start_idx + self.sequence_length]\n",
        "\n",
        "            # Calculate IQR\n",
        "            q1, q3 = np.percentile(sequence, [25, 75])\n",
        "            iqr = q3 - q1\n",
        "\n",
        "            # Add IQR to the sequence\n",
        "            sequence_with_iqr = np.append(sequence, iqr)\n",
        "\n",
        "            # Normalize and reshape\n",
        "            sequence_with_iqr = self.scaler.transform(sequence_with_iqr.reshape(1, -1))\n",
        "            sequence_with_iqr = sequence_with_iqr.reshape(1, self.sequence_length + 1, 1)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = self.model.predict(sequence_with_iqr)[0][0]\n",
        "\n",
        "            # Use IQR information to refine prediction\n",
        "            if 0 <= iqr <= 2:\n",
        "                gesture = \"Pronation\"\n",
        "                confidence = max(1 - prediction, 0.9)  # Increase confidence for pronation\n",
        "            elif iqr == 0:\n",
        "                gesture = \"Supination\"\n",
        "                confidence = max(prediction, 0.9)  # Increase confidence for supination\n",
        "            else:\n",
        "                gesture = \"Supination\" if prediction >= 0.5 else \"Pronation\"\n",
        "                confidence = prediction if prediction >= 0.5 else 1 - prediction\n",
        "\n",
        "            # Load appropriate image\n",
        "            image_path = self.dataset_paths['images'][gesture.lower()]\n",
        "            img = Image.open(image_path)\n",
        "\n",
        "            return img, f\"Predicted Gesture: {gesture} (Confidence: {confidence*100:.2f}%)\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"Error processing file: {str(e)}\"\n",
        "\n",
        "    def plot_training_history(self, history):\n",
        "        \"\"\"Plot training metrics\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        ax1.plot(history.history['accuracy'])\n",
        "        ax1.plot(history.history['val_accuracy'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Train', 'Validation'])\n",
        "\n",
        "        ax2.plot(history.history['loss'])\n",
        "        ax2.plot(history.history['val_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Train', 'Validation'])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "    classifier = EMGGestureClassifier()\n",
        "\n",
        "    if os.path.exists(classifier.model_path):\n",
        "        print(\"Loading saved model...\")\n",
        "        classifier.load_saved_model()\n",
        "    else:\n",
        "        print(\"Training new model...\")\n",
        "        X, y = classifier.load_and_preprocess_data()\n",
        "        history, test_data = classifier.train_model(X, y)\n",
        "        classifier.plot_training_history(history)\n",
        "\n",
        "    interface = gr.Interface(\n",
        "        fn=classifier.predict_gesture,\n",
        "        inputs=gr.File(label=\"Upload EMG CSV File\"),\n",
        "        outputs=[\n",
        "            gr.Image(type=\"pil\", label=\"Gesture Image\"),\n",
        "            gr.Textbox(label=\"Prediction\")\n",
        "        ],\n",
        "        title=\"EMG Gesture Classifier\",\n",
        "        description=\"Upload a CSV file containing EMG data to predict if the gesture is pronation or supination.\"\n",
        "    )\n",
        "\n",
        "    interface.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "X0042gjlD1Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "class EMGGestureClassifier:\n",
        "    def __init__(self):\n",
        "        self.dataset_paths = {\n",
        "            '90_degree': [\n",
        "                '90_degree/updated_arpit 90 degree grip test 6 (1).csv',\n",
        "                '90_degree/updated_arpit 90 degree grip test 7 (1).csv',\n",
        "                '90_degree/updated_arpit 90 degree grip test 1.csv',\n",
        "                '90_degree/updated_arpit 90 degree grip test 2.csv',\n",
        "                '90_degree/updated_arpit 90 degree grip test 4 (1).csv',\n",
        "                '90_degree/updated_arpit 90 degree grip test 5 (1).csv'\n",
        "            ],\n",
        "            'images': {\n",
        "                '90_degree_open': '90_degree/90_deg_open_img.avif',\n",
        "                '90_degree_grip': '90_degree/90_grip.jpg'\n",
        "            }\n",
        "        }\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.sequence_length = 100\n",
        "        self.model_path = 'emg_gesture_90_degree_model.h5'\n",
        "        self.scaler_path = 'emg_scaler_90_degree.pkl'\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "        \"\"\"Load and preprocess 90-degree EMG data\"\"\"\n",
        "        all_data = []\n",
        "        all_labels = []\n",
        "\n",
        "        for file_path in self.dataset_paths['90_degree']:\n",
        "            try:\n",
        "                df = pd.read_csv(file_path)\n",
        "                df['emg_signals'] = pd.to_numeric(df['emg_signals'], errors='coerce')\n",
        "                df.dropna(subset=['emg_signals'], inplace=True)\n",
        "\n",
        "                signals = df['emg_signals'].values\n",
        "                for i in range(0, len(signals) - self.sequence_length + 1, 50):\n",
        "                    sequence = signals[i:i + self.sequence_length]\n",
        "\n",
        "                    # IQR Calculation\n",
        "                    q1, q3 = np.percentile(sequence, [25, 75])\n",
        "                    iqr = q3 - q1\n",
        "\n",
        "                    sequence_with_iqr = np.append(sequence, iqr)\n",
        "                    all_data.append(sequence_with_iqr)\n",
        "                    all_labels.append(0)  # 90-degree label\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        X = np.array(all_data)\n",
        "        y = np.array(all_labels)\n",
        "\n",
        "        X = self.scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "        X = X.reshape(X.shape[0], self.sequence_length + 1, 1)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def predict_gesture(self, csv_file):\n",
        "        \"\"\"Predict gesture from uploaded 90-degree CSV file\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file.name)\n",
        "            signals = pd.to_numeric(df['emg_signals'], errors='coerce').dropna().values\n",
        "\n",
        "            if len(signals) < self.sequence_length:\n",
        "                return None, \"Error: Not enough data points in the input file\"\n",
        "\n",
        "            start_idx = (len(signals) - self.sequence_length) // 2\n",
        "            sequence = signals[start_idx:start_idx + self.sequence_length]\n",
        "\n",
        "            q1, q3 = np.percentile(sequence, [25, 75])\n",
        "            iqr = q3 - q1\n",
        "\n",
        "            sequence_with_iqr = np.append(sequence, iqr)\n",
        "            sequence_with_iqr = self.scaler.transform(sequence_with_iqr.reshape(1, -1))\n",
        "            sequence_with_iqr = sequence_with_iqr.reshape(1, self.sequence_length + 1, 1)\n",
        "\n",
        "            prediction = self.model.predict(sequence_with_iqr)[0][0]\n",
        "\n",
        "            gesture = \"90-degree grip\" if prediction >= 0.5 else \"90-degree open\"\n",
        "            confidence = max(prediction, 1 - prediction)\n",
        "\n",
        "            image_path = self.dataset_paths['images']['90_degree_grip'] if prediction >= 0.5 else self.dataset_paths['images']['90_degree_open']\n",
        "            img = Image.open(image_path)\n",
        "\n",
        "            return img, f\"Predicted Gesture: {gesture} (Confidence: {confidence*100:.2f}%)\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"Error processing file: {str(e)}\"\n",
        "\n",
        "def main():\n",
        "    classifier = EMGGestureClassifier()\n",
        "\n",
        "    if os.path.exists(classifier.model_path):\n",
        "        print(\"Loading saved model...\")\n",
        "        classifier.load_saved_model()\n",
        "    else:\n",
        "        print(\"Training new model...\")\n",
        "        X, y = classifier.load_and_preprocess_data()\n",
        "        classifier.train_model(X, y)\n",
        "\n",
        "    interface = gr.Interface(\n",
        "        fn=classifier.predict_gesture,\n",
        "        inputs=gr.File(label=\"Upload 90-degree EMG CSV File\"),\n",
        "        outputs=[\n",
        "            gr.Image(type=\"pil\", label=\"Gesture Image\"),\n",
        "            gr.Textbox(label=\"Prediction\")\n",
        "        ],\n",
        "        title=\"EMG Gesture Classifier (90 Degrees)\",\n",
        "        description=\"Upload a CSV file containing EMG data to predict 90-degree grip or open.\"\n",
        "    )\n",
        "\n",
        "    interface.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "1SmdEZRwMHeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EMGGestureClassifier:\n",
        "    def __init__(self):\n",
        "        self.dataset_paths = {\n",
        "            '180_degree': [\n",
        "                '180_degree/updated_arpit 180 degree grip test 7.csv',\n",
        "                '180_degree/updated_arpit 180 degree grip test 6.csv',\n",
        "                '180_degree/updated_arpit 180 degree grip test 5.csv',\n",
        "                '180_degree/updated_arpit 180 degree grip test 4.csv',\n",
        "                '180_degree/updated_arpit 180 degree grip test 3.csv',\n",
        "                '180_degree/updated_arpit 180 degree grip test 1.csv'\n",
        "            ],\n",
        "            'images': {\n",
        "                '180_degree_open': '180_degree/90_deg_open_img.avif',\n",
        "                '180_degree_grip': '180_degree/90_grip.jpg'\n",
        "            }\n",
        "        }\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.sequence_length = 100\n",
        "        self.model_path = 'emg_gesture_180_degree_model.h5'\n",
        "        self.scaler_path = 'emg_scaler_180_degree.pkl'\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "        \"\"\"Load and preprocess 180-degree EMG data\"\"\"\n",
        "        all_data = []\n",
        "        all_labels = []\n",
        "\n",
        "        for file_path in self.dataset_paths['180_degree']:\n",
        "            try:\n",
        "                df = pd.read_csv(file_path)\n",
        "                df['emg_signals'] = pd.to_numeric(df['emg_signals'], errors='coerce')\n",
        "                df.dropna(subset=['emg_signals'], inplace=True)\n",
        "\n",
        "                signals = df['emg_signals'].values\n",
        "                for i in range(0, len(signals) - self.sequence_length + 1, 50):\n",
        "                    sequence = signals[i:i + self.sequence_length]\n",
        "\n",
        "                    q1, q3 = np.percentile(sequence, [25, 75])\n",
        "                    iqr = q3 - q1\n",
        "\n",
        "                    sequence_with_iqr = np.append(sequence, iqr)\n",
        "                    all_data.append(sequence_with_iqr)\n",
        "                    all_labels.append(1)  # 180-degree label\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        X = np.array(all_data)\n",
        "        y = np.array(all_labels)\n",
        "\n",
        "        X = self.scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "        X = X.reshape(X.shape[0], self.sequence_length + 1, 1)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "def main():\n",
        "    classifier = EMGGestureClassifier()\n",
        "\n",
        "    if os.path.exists(classifier.model_path):\n",
        "        print(\"Loading saved model...\")\n",
        "        classifier.load_saved_model()\n",
        "    else:\n",
        "        print(\"Training new model...\")\n",
        "        X, y = classifier.load_and_preprocess_data()\n",
        "        classifier.train_model(X, y)\n",
        "\n",
        "    interface = gr.Interface(\n",
        "        fn=classifier.predict_gesture,\n",
        "        inputs=gr.File(label=\"Upload 180-degree EMG CSV File\"),\n",
        "        outputs=[\n",
        "            gr.Image(type=\"pil\", label=\"Gesture Image\"),\n",
        "            gr.Textbox(label=\"Prediction\")\n",
        "        ],\n",
        "        title=\"EMG Gesture Classifier (180 Degrees)\",\n",
        "        description=\"Upload a CSV file containing EMG data to predict 180-degree grip or open.\"\n",
        "    )\n",
        "\n",
        "    interface.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "UG5X-XVUpdYh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}